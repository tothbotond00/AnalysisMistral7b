{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb743816",
   "metadata": {},
   "source": [
    "# Alternative: Running Mistral 7B Without CUDA\n",
    "If you do not have a CUDA-enabled GPU, or if you are facing issues with PyTorch/xFormers, this notebook provides an alternative way to run Mistral 7B using `llama.cpp`. This approach works on both **CPU and low-VRAM GPUs** by using a **quantized GGUF model** that requires significantly less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240c153",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "Before proceeding, ensure that you have **CMake** and a compiler installed. On Linux/macOS, you need `make`, while on Windows, `CMake` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update && sudo apt install -y build-essential cmake git wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2020013",
   "metadata": {},
   "source": [
    "## Step 2: Clone and Compile `llama.cpp`\n",
    "This step ensures that `llama.cpp` is properly built for running Mistral 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp && rm -rf build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f51044",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake -B build\n",
    "!cmake --build build --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b2d9d",
   "metadata": {},
   "source": [
    "## Step 3: Download a Quantized Mistral Model\n",
    "We will use a **4-bit quantized model** in GGUF format, which works well on CPUs and low-VRAM GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456290c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf -P models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255df10",
   "metadata": {},
   "source": [
    "## Step 4: Run Mistral 7B Using `llama.cpp`\n",
    "Once compiled, run the following command to start the model in interactive mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!build/bin/main -m models/mistral-7b-v0.1.Q4_K_M.gguf --interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0fb39",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Success!\n",
    "You should now be able to run Mistral 7B on your **CPU or low-VRAM GPU** without requiring CUDA, PyTorch, or xFormers."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}